\documentclass[12pt]{article}

\usepackage{mystyle}

\title{Relation Extraction}
\author{
Justin T. Chiu
}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
Recent relation extraction systems predict the relationship between
an entity and value given the positions of their mentions in the text.
This requires words to be annotated as mentions.
The cost of obtaining human annotations for each word scales
linearly with the size of the text.
Automatic annotation methods allow the annotation process to scale sublinearly
in human effort, but may introduce noise due to incorrect annotations.
In order to train a probabilistic information extraction model without mention
annotations, we specify a model that, identifies important words 
and uses them to explain a triple from a knowledge base.
\end{abstract}

\section{Problem Statement}
\begin{comment}
NOTE on RE vs KBP: KBP focuses on modeling the full database, whereas
in RE the emphasis is on explaining extractions.
KBP usually has a more complicated model over the KB,
while RE sometimes makes simplifying assumptions and could make a poor
KBP system.
Example: if triple double appears in text and someone scored < 10 PTS,
then it's more likely they more likely had > 10 REB.
The difference between RE and KBP is subtle. Both fall under
information extraction, and the models may overlap in many ways.
In fact, latent variable model approaches to RE are usually able to perform KBP as well.
However, the focus is usually not on having good aggregate extraction metrics,
but rather about identifying where facts are in text.
This leads to simplifying assumptions in the model of the KB itself.
On the other hand, KBP moves the difficulty from explaining extractions to
faithfully modeling the KB.
Typically this results in a challenging inference task, as the structure
of the KB may be very complex.
\end{comment}

Relation extraction aims to extract facts from a passage of text.
Extraction systems convert facts expressed in natural language into a form
amenable to computation.
Facts consist of three components: Entities, relation types, and values.
%(Example would be fastest to clear up what a relation type is)
The challenge is to not only extract facts from text, but also
justify the extractions by determining where those facts are mentioned.

A \textit{mention} is a surface realization of an abstract object in text.
In relation extraction we justify extractions by identifying fact mentions.
As text is noisy, the realization of a fact may be difficult to locate.
We focus on locating fact mentions at the word level by identifying
individual words as value mentions, rather than entity or type mentions.
%(Why? Justify with generative model)

\begin{comment}
Note on related work:
Except for \citet{zeng2018copy}, prior work has either assumed that the locations of
entities and values are given as input features or that the locations of entities and values
are observed at training time.
\end{comment}

The problem description is as follows, focusing on the domain of basketball summaries:
Given a written summary of a basketball game $x = x_1, \ldots, x_{I}$
of length $I$,
we model the aligned box score $\set{(e_j, t_j, v_j)}_{J=1}^J$
consisting of entities $e_j$,
relation types $t_j$, and all values $v_j \in \mcV$.
The set of facts is our knowledge base (KB), which contains $J$ facts.

Let $D = \set{(e_j, t_j)}_{j=1}^J$ and $v = \set{v_j}_{j=1}^J$.
The KB $(D,v)$ can be viewed as a data table where
$d$ defines a flattened representation of the rows and columns,
the \textit{cells} of the table, and
$v$ gives the \textit{values} of the cells.
Our goal is to locate and extract facts from $x$.


Modeling only the KB $(d,v)$ given the text $x$
is not sufficient, as our goal is to locate fact mentions.
In fact, we assume the KB contains many more facts than those mentioned in the text.
This fits many scenarios in real world applications:
we may have many entity and type pairs in our data table,
but a summary may discuss only a small, salient subset of players and statistics.
We therefore propose a model that first identifies words as value mentions,
aligns those mentions to an entity and relation type in order to obtain a fact, 
and then aggregates word level decisions to resolve conflicts.
%We choose to identify value mentions as the location of a fact as entity mentions
%may be obscured through coreference and relation types may not be mentioned explicitly.

\section{Model}
We define a graphical model that performs extraction with justification.
\subsection{Notation}
Consider the following example:
Let our KB consist of the data table and values
$$
D = \left\{\begin{array}{c}
    (e_1 = \textrm{John Doe}, t_1 = \textrm{Points}), \\
    (e_2 = \textrm{John Doe}, t_2 = \textrm{Rebounds}), \\
    (e_3 = \textrm{John Doe}, t_3 = \textrm{First name}),\\
    (e_4 = \textrm{John Doe}, t_4 = \textrm{Last name}), \\
    \vdots
\end{array}\right\},
v = \left\{\begin{array}{c}
v_1 = 8,\\
v_2 = 12,\\
v_3 = \textrm{John},\\
v_4 = \textrm{Doe},\\
\vdots 
\end{array}\right\}
$$
aligned to the summary $x = \textrm{John Doe scored eight points}$.
We introduce the following latent variables:
\begin{itemize}
\item $m = m_1, \ldots, m_I$ where each $m_i \in \set{0,1}$
    indicates whether word $x_i$ is part of a value mention.
    In our example, we have $m_4 = 1$ as $x_4 = \textrm{eight}$
    mentions the value $v_1 = 8$.
    Individual words may be part of a multiword value mention.
    We aim to identify at least one word in a multiword mention.
\item $a = a_1, \ldots, a_I$ where each $a_i \in \set{1, \ldots, J}$
    gives the index of the cell of the data table $D_{a_i}$ whose
    value $v_{a_i}$ is mentioned at index $i$.
    Again examining $x_4 = \textrm{eight}$, we would have 
    $a_4 = 1$, as $x_4$ is aligned to the the cell $D_{1}$
    whose value $v_1 = 8$ is mentioned.
\item $z = z_1, \ldots, z_I$ where each $z_i \in \mcV$
    translates a value mention into a value.
    In the case of $x_4 = \textrm{eight}$, the mention must
    be translated to the value $z_4 = 8$.
    This is the canonical representation of
    a value as determined by the schema of the KB.
    For example, the KB may only store numbers in numerical form,
    as opposed to alphabetical.
\end{itemize}

\subsection{Model}
We proceed to detail the model in two stages.
We first give the word-level process, which makes a series of choices
for each word $x_i$ in the text $x = x_1, \ldots, x_I$,
then the sequence-level process which aggregates all of the
word-level decisions.

The word-level process has three steps.
For each index in time $i \in 1, \ldots, I$ we perform
\begin{enumerate}
\item Value mention identification: Given a sequence of words $x$,
    we independently choose whether each word is a value mention with
    \begin{equation}
    p(m \mid x) = \prod_i p(m_i \mid x)
    \end{equation}
\item Alignment: Each word $x_i$ is independently aligned to a 
    record in the knowledge base with
    \begin{equation}
    p(a \mid x,D) = \prod_i p(a_i \mid x,D)
    \end{equation}
    We align the word $x_i$ by choosing the cell whose value
    generates the possible value mention at index $i$.
    In particular, $a_i = j$ denotes the alignment to the cell $d_{j}$.
    We assume that each value mention aligns to a single record.
    Although we align every word to a cell, words that are not
    chosen to be mentions, i.e. $x_i$ such that $m_i = 0$,
    will be ignored.
    \item Translation: All words are translated
    into a value from the KB schema with
    \begin{equation}
    p(z \mid x) = \prod_i p(z_i \mid x)
    \end{equation}
    with $z_i \in \mcV$.
    We would like this to capture synonyms of 
    values not captured by the limited schema of the KB.
    Although every word is translated to a value, we ignore
    those that are not chosen as value mentions.
\end{enumerate}

Finally, we aggregate the word-level choices at the sequence level in order
to make a single choices for the values $v$ given the summary $x$.
\begin{enumerate}
\setcounter{enumi}{3}
\item Aggregation:
    Given the word-level values $z$, alignments $a$, mentions $m$,
    and data table $D$ we choose the sequence-level values $v_j$ independently from
    \begin{equation}
    p(v \mid z,a,m,d) = \prod_j p(v_j \mid z,a,m,D_j)
    \end{equation}
    If any of the word-level choices disagree, for example if 
    $z_1,a_1,m_1$ indicates that John scored 19 points,
    but $z_2,a_2,m_2$ indicates John scored 18, 
    we must aggregate these choices into a single distribution over values.
\end{enumerate}

We assume each of the word-level choices in each of $z,a,m$ are made independently
given the text $x$ and table cells $D$.
This gives us the following factorization of the relation extraction system:
\begin{equation}
\label{eqn:prob}
\begin{aligned}
p(v,z,a,m\mid x,D)
&= p(v\mid z,a,m,x,d)p(z\mid x)p(a\mid x,D)p(m\mid x)\\
&= \left(\prod_{j=1}^J p(v_j\mid z,a,m,x,D)\right)
\left(\prod_{i=1}^I p(z_i\mid x)p(a_i\mid x,D)p(m_i\mid x)\right)
\end{aligned}
\end{equation}

\begin{figure}[h]
\begin{center}
\resizebox {.35\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Y) {$x$};
\node[latent, below=of Y](A) {$a_i$};
\node[latent, left=of A](C) {$m_i$};
\node[latent, right=of A](Vi) {$z_i$};

\node[obs, below=of A] (D) {$D_j$};
\node[obs, below=of Vi] (V) {$v_j$};

\plate {i} {(C)(A)(Vi)} {$I$};
\plate  {j} {(D)(V)} {$J$};

\draw[->] (Y) -- (C);
\draw[->] (Y) -- (A);
\draw[->] (Y) -- (Vi);
%\draw[->] (A) -- (Vi);
\draw[->] (D) -- (A);
%\draw[-] (Vi) -- (V);
%\draw[-] (A) -- (V);
%\draw[-] (C) -- (V);
%\draw[-] (E) to [bend right = 25] (V);
%\draw[-] (T) -- (V);
\draw[->] (Vi) -- (V);
\draw[->] (C) -- (V);
\draw[->] (A) -- (V);
\draw[->] (D) -- (V);

\end{tikzpicture}
} %% end resize
\end{center}
\caption{Our model predicts word-level values and alignments
then aggregates those choices over all indices $i$ to
make a single decision for each value.
Each word has the following latent variables:
the mention $m_i \in \set{0,1}$ indicates whether word $x_i$ is a value mention,
the alignment $a_i$ gives the cell $D_{a_i}$ that $x_i$ aligns to,
and the value $z_i$ gives the canonical value that $x_i$ translates to.
}
\label{fig:infmodel}
\end{figure}

\subsection{Parameterization}
We parameterize the conditional distributions for
mention identification, alignment, translation, and aggregation below.

Let $\bh_i \in \R^d$ be a contextual embedding of the word $x_i$,
and $E$ an embedding function that maps the cells of the data table $D$
to vectors in $\R^{d}$.
\begin{enumerate}
\item Identification: We use the contextual embedding to directly predict
whether a word $x_i$ is part of a value mention.
\begin{equation}
p(m_i \mid x) \propto \exp(W_m\bh_i), W_m \in \R^{2 \times d}
\end{equation}
\item Alignment: We use a bilinear function of the cell embeddings and
contextual embeddings to parameterize the alignment distribution.
\begin{equation}
p(a_i \mid x,d) \propto \exp(E(D_{a_i})^T W_d \bh_i)\\
\end{equation}
with $W_d \in \R^{d \times d}$.
\item Translation: We use the contextual embedding to translate a word
into a value.
\begin{equation}
p(z_i \mid x) \propto \exp(W_z \bh_i), W_z \in \R^{|\mcV| \times d}
\end{equation}
\item Aggregation:
We parameterize a distribution over the value $v_j$ of cell $D_j$
by counting the votes of relevant word-level values.
In order for a word $x_i$ to vote on a value $v_j$
the word must be a mention such that $m_i = 1$,
it must be aligned to the correct cell $a_i = j$,
and its translation must match $z_i = v_j$.
Once the votes are collected,
they are aggregated then normalized into a distribution.

If no word votes for any values for cell $D_j$, i.e. there does not exist
a word $x_i$ such that $a_i = j$ and $m_i = 1$,
we choose the value given only the cell
$p(v_j \mid d_j) \propto \exp(E(v_j)^TW_v [E(D_j)])$.

Formally, the conditional distribution is given by
\begin{align}
p(v_j \mid z,a,m,d) &\propto \begin{cases}
    \prod \exp(\psi(v_j, z_i,a_i,m_i,d)),  & \exists i, m_i = 1 \wedge a_i = j\\
    %p(v_j \mid e_j,t_j), & \textrm{otherwise}
    \exp(E(v_j)^TW_v [E(d_j)]), & \textrm{otherwise}
\end{cases}\\
\psi(v_j, z_i, a_i, m_i,d) &= \mathbf{1}(v_j = z_i, a_i = j, m_i=1)%\\
%p(v_j \mid e,t) & \propto \exp(E(v_j)^TW_v [E(e),E(t)])
\end{align}
\end{enumerate}

\section{Training and Inference}
To train a latent variable model, we must marginalize over the unobserved RVs
and maximize the likelihood of the observed.
Although the latent variables $z,a,m$ are important, as they indicate
where and what facts are contained in the text, they are not observed
and must be marginalized over.
Therefore we would ideally optimize the following objective
\begin{equation}
\log p(v \mid x,d) = \log \sum_{z,a,m} p(v,z,a,m \mid x,d)
\end{equation}
However, maximizing $\log p(v \mid x,d)$ directly is very expensive for this model
as the summation over $z,a,m$ is intractable;
in particular,
the summation over $z,a,m$ has computational complexity $O((|\mcV|\cdot J\cdot 2)^{I})$.

We therefore resort to approximate inference,
specifically amortized variational inference.

\subsection{Inference network}
We introduce an inference network $q(z,a,m\mid v,x,d)$
and optimize the following lower bound on the marginal likelihood
with respect to the parameters of both $p$ and $q$:
\begin{equation}
\label{eqn:lowerbound}
\log p(v\mid x) \geq
\Es{q(z,a,m\mid v,x,d)}{\log \frac{p(v,z,a,m\mid x,d)}{q(z,a,m\mid v,x,d)}}
\end{equation}


Recall that $\bh_i \in \R^d$ is a contextual embedding of the word $x_i$.
We introduce the following attention weights over records
in order to get a weighted representation
of the KB for each index $i$:
\begin{align}
\bg_{r_j} &= [E(d_j), E(v_j)]\\
\alpha_j &\propto \exp(\bg_{r_j}^T W_\alpha \bh_i)
\end{align}

We propose to parameterize the inference network $q(z,a,m\mid v,x,d)$ as follows:
\begin{enumerate}
\item Identification: Each distribution $q(m_i \mid v,x)$ 
    has access to the values $v$ from the KB, which it conditions on
    when detecting value mentions. 
    \begin{equation}
    p(m_i \mid v,x) =
    V_m \textrm{MLP}([\sum_j \alpha_j \cdot \bg_{r_j}, \bh_i]), V_m \in \R^{2 \times d}
    \end{equation}
    where MLP is a neural network.
\item Alignment: The alignment model $q(a_i \mid v,x,d)$
    uses the attention weights to parameterize the alignment
    $p(a_i \mid x) = \alpha_{a_i}$.
\item Translation: $q(z_i \mid a,v,x) = \mathbf{1}(z_i = v_{a_i})$
    conditions on the alignments $a$ and ensures the chosen $z$ is consistent
    with the alignments. 
\end{enumerate}

We decompose 
\begin{equation}
\label{eqn:elbo}
\begin{aligned}
q(z,a,m\mid v,x,d) &= q(z \mid a,v,x)q(a\mid v,x,d)q(m \mid v,x)\\
&= \prod_i q(z_i \mid a,v,x)q(a_i \mid v,x,d)q(m_i \mid v,x)
\end{aligned}
\end{equation}

We use the score function gradient estimator to perform gradient ascent on the objective
in Equation~\ref{eqn:elbo} with respect to both $p$ and $q$.
We utilize a leave-one-out baseline for variance reduction.

One concern is that the model may learn to never rely on the text for extraction,
setting $m_i = 0$ at every index.
We can avoid this by initializing $q(z)$ to ensure that for words $x_i$
where there exists a $z_i \in \mcV$ such that $x_i$ and $z_i$ are a lexical match
we assign high probability to the transliteration $q(z_i = x_i)$
via pretraining.

\section{Evaluation}
As we assumed that the KB contained a superset of the facts contained in
a sequence of text, we are interested in evaluating whether the model can discover
and locate the subset of facts that are expressed in the text.

We evaluate by determining whether the model can identify which facts in a KB
are expressed in text.
Given a ground truth set of facts extracted by a human annotator,
we compute the precision and recall of the extractions by the model.
We perform extraction by finding $\arg\max_{z_i,a_i,m_i}q(z_i),q(a_i),q(m_i)$
for each word $x_i$.


\begin{comment}

\subsection{Approximate the posterior of a generative model}
Alternatively, we may introduce a generative model of text $q(x, v)$ 
that inverts the relation extraction model $p(v \mid x)$ and optimize the following objective:
\begin{equation}
\log q(x,v) - KL[p(v,z,a,m \mid x) || q(v,z,a,m \mid x)]
\end{equation}
which entails training the generative model of text while approximating its
posterior with the information extraction system.

decompose the training of our extraction system $p(v\mid x)$ into two stages:
In the first stage we train $p(z,a,m\mid x,d)$ to approximate the posterior
of a conditional model of text given a complete KB $q(x,z,a,m \mid v,d)$.
This has the benefit of allowing us to exert control over where value mentions are detected
through our design of the text model $q$.

In the second stage, we have two choices:
a) train $p(v \mid z,a,m,x,d)$ to approximate the posterior of a full generative model of text and
the values of KB $q(x,v\mid d)$.
b) train $p(v \mid z,a,m,x,d)$ using the following lower bound:
\begin{equation}
\label{eqn:lowerbound2}
\log p(v\mid x,d) \geq
\Es{p(z,a,m\mid x,d)}{\log p(v\mid z,a,m, x,d)}
\end{equation}
Ideally the bound in Eqn.~\ref{eqn:lowerbound2}
should not be looser than the one presented in Eqn.~\ref{eqn:lowerbound},
as conditioning on the observed values of a KB should not reduce the entropy of
a good alignment model.

How do we split the gradient over time?

\end{comment}

\bibliography{bib}
\bibliographystyle{acl_natbib}

\end{document}
