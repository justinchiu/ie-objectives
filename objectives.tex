\documentclass[12pt]{article}

\usepackage{mystyle}

\title{Objectives for Information Extraction}
\author{
Justin T. Chiu
}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
Many recent information extraction systems predict the relationship between
an entity and value given the positions of their mentions in the text.
This requires observed locations of mentions, which requires annotations at the word level.
Any form of annotation at the word level does not scale as the size of the text
and the number of labels increases, and even more so if there is ambiguity.
In order to train a probabilistic information extraction model without mention
position annotations, we specify a model that, for each word,
either chooses a triple from a knowledge base to explain or chooses to explain nothing.
\end{abstract}

\section{Problem Statement}

In relation extraction we extract facts from a passage of text.
The goal of relation extraction is to convert facts expressed in natural language into a form
amenable to computation.
Relation extraction uses a relational representation: facts detail
how values are related to entities.
The challenge is to not only extract facts from text, but also
determine where those facts are mentioned.
In this work we build a relation extraction system with minimal supervision.
We assume access to a knowledge base (KB) that is aligned to sequences of text
that discuss subsets of the information in the KB.

In order to perform extraction, we must first identify where facts are mentioned
then aggregate those decisions to construct a knowledge base.
As sentences may contain many entity and value mentions, and therefore 
a large number of facts, we identify facts at a scale that minimizes ambiguity
by casting the mention identification problem as classifying whether each word is a mention.
We first identify the position of value mentions,
predict who (the entity) and what (the relation type) the value mention
is discussing, then aggregate those decisions.

Note on related work:
Except for \citet{zeng2018copy}, prior work has either assumed that the locations of
entities and values are given as input features or that the locations of entities and values
are observed at training time.

We are primarily concerned with the scenario where we have an overcomplete KB schema with
respect to a specific passage of text.
This fits many scenarios in real world applications:
we may have thousands of entities of interest if our KB was pulled from an 
external source such as Freebase,
but the particular document we wish to analyze only discusses tens of entities,
only a few of which are present in our KB.

The problem description is as follows:
given a text $x = x_0, \ldots, x_{I}$ we model the facts
$r = \set{(e_j, t_j, v_j)}_{j=0}^J$ expressed in that text
with respect to a schema that details all entities $e_j \in \mcE$,
relation types $t_j \in \mcT$, and all values $v_j \in \mcV$.
We assume that the schema of the KB $(\mcE, \mcT, \mcV)$ is known at all times,
and that the schema covers all facts of interest.
The set of facts $r$ is our knowledge base (KB),
and each individual fact $r_j$ is a record.
For brevity, let $e = \set{e_j}_{j=0}^J, t = \set{t_j}_{j=0}^J, v = \set{v_j}_{j=0}^J$
be the list of the entities, types, and values of the records in $r$ respectively.

Given all entities $e$ and types $t$,
we reduce the construction of $r$ to predicting, for every $j\in 0,\ldots,J$,
the value $v_j$ corresponding to the entity $e_j$ and type $t_j$.
We propose a model for this distribution $p(v \mid x, e, t)$.

\section{Model}
We define a graphical model that jointly models 
word and KB level extraction. 
The model first extracts information at the word level,
then aggregates its predictions at the sequence level:

The word level extraction process has three steps.
For each index $i \in 0, \ldots, I$
\begin{enumerate}
\item Value mention identification: Given a sequence of words $x$,
    we identify whether each word is a value mention with
    $p(m \mid x) = \prod_i p(m_i \mid x)$.
    Each $m_i \in \set{0,1}$.
    Not every word in a mention must be identified; it suffices to find
    at least 1 word in a value mention.
\item Alignment: Each value mention is then aligned to a 
    record in the knowledge base with $p(a \mid x) = \prod_i p(a_i \mid x,e,t)$,
    We align the word $y_i$ by choosing who (the entity)
    and what (the relation type) generate the possible value mention at index $i$.
    In particular, $a_i = j$ denotes the alignment to the record $r_{j}$
    with $a_i \in 0, \ldots, J$.
    We assume that each value mention aligns to a single record.
\item Translation: All value mentions are translated
    into a representation from the KB schema with
    $p(z \mid x) = \prod_i p(z_i \mid x)$, with $z \in \mcV$.
\end{enumerate}
We choose to locally normalize the word level distributions as our goal is to
extract information from text, not condition on an existing KB.

Finally, in case there is disagreement on values at the word level,
we aggregate the word level information at the sequence level in order
to give a single distribution over the record values for $x$.
\begin{enumerate}
\setcounter{enumi}{3}
\item Aggregation $p(v \mid z,a,m) = \prod_j p(v_j \mid z,a,m)$:
    Given the word level values $z$, alignments $a$, value mention decisions $m$,
    we choose the sequence level value $v_j$.
\end{enumerate}

\begin{figure}[h]
\begin{center}
\resizebox {.3\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Y) {$x$};
\node[latent, below=of Y](A) {$a_i$};
\node[latent, left=of A](C) {$m_i$};
\node[latent, right=of A](Vi) {$z_i$};

\node[obs, below=of C] (E) {$e_j$};
\node[obs, below=of A] (T) {$t_j$};
\node[obs, below=of Vi] (V) {$v_j$};

\plate {i} {(C)(A)(Vi)} {$I$};
\plate  {j} {(E)(T)(V)} {$J$};

\draw[->] (Y) -- (C);
\draw[->] (Y) -- (A);
\draw[->] (Y) -- (Vi);
%\draw[->] (A) -- (Vi);
\draw[->] (E) -- (A);
\draw[->] (T) -- (A);
\draw[-] (Vi) -- (V);
\draw[-] (A) -- (V);
\draw[-] (C) -- (V);
\draw[-] (E) to [bend right = 25] (V);
\draw[-] (T) -- (V);

\end{tikzpicture}
} %% end resize
\end{center}
\caption{Our model predicts word-level values and alignments
then aggregates those choices over all indices $i$ to
predict values at the KB level.
}
\label{fig:infmodel}
\end{figure}

This gives us the following factorization of the relation extraction system:
\begin{equation}
\label{eqn:prob}
\begin{aligned}
p(v \mid x,e,t) &= \sum_{z,a,m} p(v,z,a,m\mid x,e,t)\\
&= \sum_{z,a,m} p(v\mid z,a,m,x,e,t) \prod_i p(z_i, a_i, m_i\mid x,e,t)\\
&= \sum_{z,a,m} \prod_j p(v_j\mid z,a,m,x,e,t) \prod_i p(z_i\mid x)p(a_i\mid x,e,t)p(m_i\mid x)\\
\end{aligned}
\end{equation}

\subsection{Parameterization}
Our model has four steps: mention identification, mention alignment, 
mention translation, and aggregation.
We parameterize the conditional distributions of each step below.

Let $\bh_i \in \R^d$ be a contextual embedding of the word $x_i$,
and $E$ an embedding function that maps entities and types
to vectors in $\R^{d'}$.
\begin{enumerate}
\item Identification: $p(m_i \mid x) \propto \exp(W_m\bh_i), W_m \in \R^{2 \times d}$
\item Alignment: $p(a_i \mid x) \propto \exp([E(e_{a_i}), E(t_{a_i})]^TW_a \bh_i),
W_a \in \R^{2d' \times d}$ 
\item Translation: $p(z_i \mid x) \propto \exp(W_z \bh_i), W_z \in \R^{|\mcV| \times d}$
\item Aggregation: 
$$p(v_j \mid z,a,m,e,t) \propto \prod_i \exp(\psi(v_j, z_i, a_i, m_i,e,t)),$$
with
$$
\psi(v_j, z_i, a_i, m_i,e,t) = \begin{cases}
f_v(v_j,e,t), & m_i = 0 \\
1(v_j = z_i, a_i = j), & m_i = 1.
\end{cases}
$$
\end{enumerate}

\section{Training}
To train a latent variable model, we must marginalize over the unobserved RVs
and maximize the likelihood of the observed.
However, maximizing $\log p(v \mid x)$ directly is very expensive for this model.
Marginalizing over just $z$ would be $O(|\mcV|^{I})$, which is exponential in
the length of the text.

We therefore resort to approximate inference,
specifically amortized variational inference.

\subsection{Inference network}
Our first approach is to specify an inference network $q(z,a,m\mid v,x,e,t)$.
We then optimize the following lower bound on the marginal likelihood
with respect to the parameters of both $p$ and $q$:
\begin{equation}
\label{eqn:lowerbound}
\log p(v\mid x) \geq
\Es{q(z,a,m\mid v,x,e,t)}{\log \frac{p(v,z,a,m\mid x,e,t)}{q(z,a,m\mid v,x,e,t)}}
\end{equation}

We propose to parameterize $q(z,a,m\mid v,x,e,t)$ as follows.
We decompose 
\begin{equation}
\begin{aligned}
q(z,a,m\mid v,x,e,t) &= q(z \mid a,v,x)q(a\mid v,x,e,t)q(m \mid v,x)\\
&= \prod_i q(z_i \mid a,v,x)q(a_i \mid v,x,e,t)q(m_i \mid v,x)
\end{aligned}
\end{equation}
The conditional distributions of our inference network
are very similar to the relation extraction model,
but they condition on the values $v$.
\begin{enumerate}
\item The value mention model $q(m_i \mid v,x) = \Bern(g_{m}(v,x)_i)$ 
    has access to the values $v$ from the KB, which it conditions on
    when detecting value mentions. 
\item The alignment model $q(a_i \mid v,x,e,t) = \Cat(g_{a}(v,x,e,t))$
    uses a contextual representation of each $x_i$ and chooses a record.
    In contrast to $p(a\mid x,e,t)$, this model has access to values as well.
\item The translation model $q(z_i \mid a,v,x) = 1(z_i = v_{a_i})$
    conditions on the alignments $a$ and ensures the chosen $z$ is consistent
    with the alignments. 
\end{enumerate}

One concern is that the model may learn to never rely on the text for extraction,
setting $m_i = 0$ at every index.
We can avoid this by initializing $q(z)$ to ensure that for words $x \in \mcV$ 
we have $q(z = x)$ is high, biasing the translation model towards transliteration
at the start of training.

\subsection{Approximate the posterior of a generative model}
Alternatively, we may decompose the training of our extraction system $p(v\mid x)$ into two stages:
In the first stage we train $p(z,a,m\mid x,e,t)$ to approximate the posterior
of a conditional model of text given a complete KB $q(x,z,a,m \mid e,t,v)$.
This has the benefit of allowing us to exert control over where value mentions are detected
through our design of the text model $q$.

In the second stage, we have two choices:
a) train $p(v \mid z,a,m,x,e,t)$ to approximate the posterior of a full generative model of text and
the values of KB $q(x,v\mid e,t)$.
b) train $p(v \mid z,a,m,x,e,t)$ using the following lower bound:
\begin{equation}
\label{eqn:lowerbound2}
\log p(v\mid x) \geq
\Es{p(z,a,m\mid x,e,t)}{\log p(v\mid z,a,m, x,e,t)}
\end{equation}
Ideally the bound in Eqn.~\ref{eqn:lowerbound2}
should not be looser than the one presented in Eqn.~\ref{eqn:lowerbound},
as conditioning on the observed values of a KB should not reduce the entropy of
a good alignment model.

\section{Evaluation}
Although we have a model over the values of all records,
evaluation does not include the final distribution over all record values.
As we assumed that the KB contained a superset of the facts contained in
a sequence of text, we are evaluate whether the model can discover the subset of facts that 
are expressed in the text.
We therefore perform extraction by using the marginal distributions
$q(z),q(a),q(c)$ to value mentions as well as entities and types,
giving us facts.



\begin{comment}
\section{Three perspectives on training}
We can either train $q$ directly on the conditional task
or train it to mimic the posterior of a suitable generative model.

Axes of objectives:
\begin{enumerate}
\item Proposal distribution: Learned or uniform (or some prior)
\item Probabilistic interpretation: Marginal likelihood or KL
\item Probabilistic interpretation 2: Approximate posterior
    of a generative model or learn directly.
\end{enumerate}

\subsection{Marginal loss}
Our first loss, the marginal likelihood, takes the form of
$$\mcL_{\textrm{ML}} = \sum_t \log \sum_{a_t} \sum_{c_t} q(a_t,c_t,v_{a_t})$$
where the unobserved alignments and latent copy are marginalized over.
This has the interpretation of maximizing the `softmax' of the 
log probabilities, where softmax is from the physics usage
as a smooth approximation to the max.
However, the sharpness of is limited by the choice of $q$.
With a uniform $q(a,c)$, $\mcL = \sum_t \sum_{a_t}q(v_{a_t}) + C$,
where $C$ is the normalization term which we can assume to be constant here.
The gradient of this term wrt the log probabilities is weighted by the posterior,
so a high entropy $q$ will result in a smaller gradient.

WRONG! The inference network is a mixture model consisting of $q(v\mid y)$ and $p(v)$!
Derive from generative model.

\subsection{KL}
The second loss, a lower bound on the marginal likelihood, is the following
$$\mcL_{\textrm{KL}} = \sum_t \sum_{a_t} \sum_{c_t} q(a_t,c_t)\log q(v_{a_t})$$
The benefit of this loss is that we can approximate its gradient via Monte Carlo sampling.
Under a uniform $q(a,c)$, we must maximize the probabilities of all values.
With a learned $q$, the lower bound can be made much tighter so there is
less of a difference.

\subsection{Approximating the posterior of a generative model}
The information extraction model itself was inspired by the following generative process:
For every word $y_t$
\begin{enumerate}
\item Generate the values of the KB $v \sim p(v)$ assuming the schema is fixed.
\item Choose $c_t \sim \Bern(f(y_{<t})$, which determines whether to generate 
    word $y_t$ from a language model or the KB.
\item If $c_t = 0$, generate $y_t \sim \Cat(g(y_{<t}))$.
\item Otherwise pick an alignment to the KB $a_t \sim \Cat(h(y_{<t}))$
    then generate $y_t \sim \Cat(f'(v_{a_t}))$.
\end{enumerate}
We then train our model to approximate the posterior $p(v \mid y)$.
Since this generative model does not use every record during generation,
the posterior may only explain a subset of all records.
This fits our hypothesis that the KB contains a superset of records expressed in text.

\begin{figure}[t]
\begin{center}
\resizebox {.3\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Yp) {$y_{<t}$};
\node[obs, right=of Yp] (Yi) {$y_t$};
\node[latent, below=of Yp](A) {$a_t$};
\node[latent, left=of A](C) {$c_t$};
\node[latent, below=of Yi](Vi) {$v_t$};

\node[obs, below=of C] (E) {$e_i$};
\node[obs, below=of A] (T) {$t_i$};
\node[obs, below=of Vi] (V) {$v_i$};

\plate {i} {(Yp)(Yi)(C)(A)(Vi)} {$T$};
\plate  {j} {(E)(T)(V)} {$I$};

\draw[->] (Yp) -- (Yi);
\draw[->] (Yp) -- (C);
\draw[->] (Yp) -- (A);
\draw[->] (A) -- (Vi);
\draw[->] (E) -- (A);
\draw[->] (T) -- (A);
\draw[->] (V) -- (Vi);
\draw[->] (Vi) -- (Yi);
\draw[->] (C) -- (Yi);
\end{tikzpicture}
} %% end resize
\end{center}
\caption{The generative model which produces words given a knowledge base.}
\label{fig:genmodel}
\end{figure}


The loss is then given by
\begin{align*}
\mcL_{\textrm{VI}}
\end{align*}

The primary benefit of this over a purely conditional approach is semi-supervised information extraction,
where values are missing.

\begin{align}
\arg\max_p \sum_{y'}\sum_{x'} p^*(y',x')\log \frac{p(y',x')}{p^*(y',x')}
&= \arg\max_p \sum_{y'}\sum_{x'} p^*(y',x')\log \sum_z p(y',z,x')\\
&= \arg\max_p \sum_{y'}\sum_{x'} p^*(y',x')\log \sum_z p(y',z,x')\\
&\geq 
\end{align}
\end{comment}

\bibliography{bib}
\bibliographystyle{acl_natbib}

\end{document}
