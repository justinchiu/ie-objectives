\documentclass[12pt]{article}

\usepackage{mystyle}

\title{Objectives for Information Extraction}
\author{
Justin T. Chiu
}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
Many recent information extraction systems predict the relationship between an entity and value given
their locations in the text.
This requires observed locations of mentions, which requires annotations at the word level.
Any form of annotation at the word level does not scale as the size of the text
and the number of labels increases, and even more so if there is ambiguity.
In order to train a probabilistic information extraction without any
supervision at the level of text, we specify a model 
that, for each word, either chooses a triple from a knowledge base to explain
or chooses to explain nothing.
\end{abstract}

\section{Introduction}

In relation extraction we extract facts from a passage of text.
The length of the text varies: depending on the dataset we may be interested extracting
all facts expressed in a sentence or paragraph.
The datasets we use do not consider the case where all facts from a large corpus are to be extracted,
as supervision would be very expensive.
All sequences of text are bounded in length.

Given a text $y = [y_0, \ldots, y_T]$ the goal is to predict all facts
$r = \set{(e_i, t_i, v_i)}_{i \in I}$ expressed in that text.
We refer to the set of facts $r$ as a knowledge base (KB),
and each fact $r_i$ is referred to as a record.
Each record $r_i$ consists of an entity, type, and value triple. 

We assume supervision only at the proposition level of a KB.
We know what the relationship is between entities and values
(although this assumption can be relaxed),
but we do not know where in text records are realized.
Except for \citet{zeng2018copy}, prior work has either assumed that the locations of
entities and values are given as input features or that the locations of entities and values
are observed at training time.
We relax this assumption and do not use any annotations at the level of text.

\begin{table}[htbp]\caption{Notation}
\centering % to have the caption near the table
\begin{tabular}{r c p{10cm} }
\toprule
$y$ & $\triangleq$ & A vector of words $y_t$\\
$r$ & $\triangleq$ & The knowledge base,
    an indexed set of records each consisting of entities, types, and values.\\
$e$ & $\triangleq$ & The list of all entities in a KB by index.\\
$t$ & $\triangleq$ & The list of all types in a KB by index.\\
$v$ & $\triangleq$ & The list of all values in a KB by index.\\
\bottomrule
\end{tabular}
\label{tab:TableOfNotationForMyResearch}
\end{table}

Our goal is to produce a set of record triples given text.
As we assume that only the KB and text are observed,
we can either produce this set directly from the text or identify realizations of
records at a more granular level and then aggregate our choices.
We choose to pursue the latter since identifying facts at a granular level
presents a step towards compositional language understanding,
i.e. shallow parsing.

We are primarily concerned with the scenario where we have an overcomplete KB schema with
respect to a specific passage of text.
This fits many scenarios in real world applications:
we may have thousands of entities of interest if our KB was pulled from an 
external source such as Freebase,
but the particular document we wish to analyze only discusses tens of entities,
only a few of which are present in our KB.

\section{Model}
We choose to identify realizations of records at the word level.
Although entity or value mentions may be multi-word expressions,
identifying realizations at the word level is no less expressive as
we can aggregate our word level predictions into sequence level ones.

Our information extraction model is denoted $q(r \mid y)$.
Our model first identifies words that are value mentions,
then aligns those words to a record in the KB by predicting what
entity and the relation type associated with the value mention.

Our word level model is given by the following:
For each word $y_t$, we have
\begin{enumerate}
\item Value mention detector $q(c_t \mid y)$: We classify whether word $y_t$
   is a value mention. 
\item Translation $q(v_t \mid y)$: We translate the word $y_t$ into a value $v_t$.
\item Alignment $q(a_t \mid y)$: We align the word $y_t$ by classifying who (entity) and what (relation type)
are being talked about.
\end{enumerate}

In order to parameterize a distribution over KB $r$, we must then aggregate all of the word-level
information.

\begin{figure}[t]
\begin{center}
\resizebox {.3\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Yp) {$y_{<i}$};
\node[obs, right=of Yp] (Yi) {$y_i$};
\node[latent, below=of Yp](A) {$a_i$};
\node[latent, left=of A](C) {$c_i$};
\node[latent, below=of Yi](Vi) {$v_i$};

\node[obs, below=of C] (E) {$e_j$};
\node[obs, below=of A] (T) {$t_j$};
\node[obs, below=of Vi] (V) {$v_j$};

\plate {i} {(Yp)(Yi)(C)(A)(Vi)} {$I$};
\plate  {j} {(E)(T)(V)} {$J$};

\draw[->] (Yp) -- (Yi);
\draw[->] (Yp) -- (C);
\draw[->] (Yp) -- (A);
\draw[->] (A) -- (Vi);
\draw[->] (E) -- (A);
\draw[->] (T) -- (A);
\draw[->] (V) -- (Vi);
\draw[->] (Vi) -- (Yi);
\draw[->] (C) -- (Yi);
\end{tikzpicture}
} %% end resize
\end{center}
\caption{The generative model which produces words given a knowledge base.}
\label{fig:genmodel}
\end{figure}

\begin{figure}[t]
\begin{center}
\resizebox {.3\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Y) {$y$};
\node[latent, below=of Y](A) {$a_i$};
\node[latent, left=of A](C) {$c_i$};
\node[latent, right=of A](Vi) {$v_i$};

\node[obs, below=of C] (E) {$e_j$};
\node[obs, below=of A] (T) {$t_j$};
\node[obs, below=of Vi] (V) {$v_j$};

\plate {i} {(C)(A)(Vi)} {$I$};
\plate  {j} {(E)(T)(V)} {$J$};

\draw[->] (Y) -- (C);
\draw[->] (Y) -- (A);
\draw[->] (Y) -- (Vi);
%\draw[->] (A) -- (Vi);
\draw[->] (E) -- (A);
\draw[->] (T) -- (A);
\draw[-] (Vi) -- (V);
\draw[-] (A) -- (V);
\draw[-] (C) -- (V);

\end{tikzpicture}
} %% end resize
\end{center}
\caption{The inference network which predicts word-level values and alignments.}
\label{fig:infmodel}
\end{figure}

\section{Three perspectives on training}
We can either train $q$ directly on the conditional task
or train it to mimic the posterior of the generative model.

\bibliography{bib}
\bibliographystyle{acl_natbib}

\end{document}
